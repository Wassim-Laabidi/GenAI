{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Wassim-Laabidi/GenAI/blob/main/GenAi_Summarization_With_Langchain_Summarize_Text_Documents_GP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YIs4bngsPm8"
      },
      "source": [
        "## <font color=blue> TASK 1: Install and setup LangChain\n",
        "Welcome to this project notebook, which will serve as your guide to constructing your inaugural Generative AI application. Within this notebook, you'll encounter concise descriptions of each task to enhance your comprehension of the sequence. Our initial task commences with the importation of essential libraries required for this project.\n",
        "\n",
        "***Note: Before importing the libraries please ensure that all the library modules such as Langchain, Streamlit, PyPdf and other required libraries are installed on this notebook by running the pip command as shown below.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wnvxw5okxYjT",
        "outputId": "7736af17-3754-4fd8-cc7f-09aaaef303c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.56)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.34)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.3)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (0.3.14)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.53 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.3.56)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.76.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain_openai) (0.3.34)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain_openai) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain_openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain_openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain_openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain_openai) (4.13.2)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.53->langchain_openai) (2.11.3)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain_openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain_openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain_openai) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.53->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain_openai) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain_openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.53->langchain_openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain_openai) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.53->langchain_openai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.4.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.22)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.56 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.56)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.24)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.34)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.56->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (2.33.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.23-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m21.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-community\n",
            "  Attempting uninstall: langchain-community\n",
            "    Found existing installation: langchain-community 0.3.22\n",
            "    Uninstalling langchain-community-0.3.22:\n",
            "      Successfully uninstalled langchain-community-0.3.22\n",
            "Successfully installed langchain-community-0.3.23\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.11/dist-packages (1.44.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (8.1.8)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (24.2)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (11.2.1)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.11/dist-packages (from streamlit) (5.29.4)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.11/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from streamlit) (4.13.2)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.11/dist-packages (from streamlit) (3.1.44)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.11/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.11/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.11/dist-packages (from altair<6,>=4.0->streamlit) (1.36.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.27->streamlit) (2025.1.31)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.24.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.11/dist-packages (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "#For installing the Langchain module associated with OpenAI LLM model\n",
        "!pip install langchain\n",
        "!pip install langchain_openai\n",
        "!pip install -U langchain-community\n",
        "#For installing the Python library responsible for PDF upload\n",
        "!pip install pypdf\n",
        "#For installing the library responsible for web app development\n",
        "!pip install streamlit\n",
        "#For installing tokeniser library that asists with converting text strings into tokens recognizable by OpenAI models\n",
        "!pip install tiktoken\n",
        "#For installing the library used for invoking the environment file containing secret API key\n",
        "!pip install python-dotenv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJ0AR0gorTlF"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import dotenv\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import streamlit as st"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvtUejaS8v8T"
      },
      "source": [
        "## <font color=blue> Load OpenAI API Key to access LLM model\n",
        "\n",
        "## <font color=black>\n",
        "1. Go to https://platform.openai.com/api-keys\n",
        "2. Click on the '+ Creat new secret key button'\n",
        "3. Enter an identifier name(optional) and click on the \"Create secret key\" button\n",
        "4. Copy the API key to be used in the API.env file that you need to upload to Google Colab environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtlBVZonvkf8"
      },
      "outputs": [],
      "source": [
        "# Load the .env file and invoke the secret API key from the file\n",
        "# dotenv.load_dotenv('API.env')\n",
        "# OpenAI.api_key = os.getenv(\"OPEN_API_KEY\")\n",
        "# Using collabs secret\n",
        "from google.colab import userdata\n",
        "OpenAI.api_key = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRjptllP7Xgb"
      },
      "source": [
        "# <font color=blue> Load PDF file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FL4FtXMkvocT"
      },
      "outputs": [],
      "source": [
        "pdf_url = \"https://www.medrxiv.org/content/10.1101/2021.07.15.21260605v1.full.pdf\"\n",
        "\n",
        "loader = PyPDFLoader(pdf_url)\n",
        "pages = loader.load_and_split()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEGnGRAe_op2",
        "outputId": "99aaa569-3f4b-4066-c309-532c77c9fa53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "#number of pages\n",
        "len(pages)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awpobT56Urc5",
        "outputId": "535607db-d76f-494f-a606-226a30212221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COVID-19 Chest X-Ray Image Classification Using Deep Learning\n",
            "Gunther Correia Bacellar,1 Mallikarjuna Chandrappa,1 Rajlakshman Kulkarni,1 Soumava Dey1* \n",
            "Department of Computer Science, University of Illinois at Urbana-Champaign, Urbana, IL, 61801, USA  \n",
            "*Correspondence: soumava2@illinois.edu; soumavadey87@gmail.com \n",
            "               \n",
            "ABSTRACT \n",
            "The rise of the coronavirus disease 2019 (COVID-19) pandemic has made it necessary to improve existing medical screening \n",
            "and clinical management of this disease. While COVID-19 patients are known to exhibit a variety of symptoms, the major \n",
            "symptoms include  fever, cough, and fatigue. Since these symptoms also appear in pneumonia patients, this creates \n",
            "complications in COVID-19 detection especially during the flu season. Early studies identified abnormalities in chest X -ray \n",
            "images of COVID -19 infected patient s that could be beneficial for disease diagnosis. Therefore, chest X -ray image -based \n",
            "disease classification has emerged as an alternative to aid medical diagnosis. However, manual detection of COVID-19 from a \n",
            "set of chest X-ray images comprising both COVID -19 and pneumonia cases is cumbersome and prone to huma n error. Thus, \n",
            "artificial intelligence techniques powered by deep learning algorithms,  which learn from radiography images and predict \n",
            "presence of COVID-19 have potential to enhance current diagnosis process. Towards this purpose, here we implemented a set \n",
            "of deep learning pre -trained models such as ResNet, VGG, Inception and EfficientNet in conjunction with developing a \n",
            "computer vision AI system based on our own convolutional neural network (CNN) model: Deep Learning in Healthcare (DLH)-\n",
            "COVID. All these CNN models cater to image classification exercise.  We used publicly available resources of 6,432 images \n",
            "and further strengthened our model by tuning hyperparameters to provide better generalization dur ing the model validation \n",
            "phase. Our final DLH-COVID model yielded the highest accuracy of 96% in detection of COVID-19 from chest X-ray images \n",
            "when compared to images of both pneumonia-affected and healthy individuals. Given the practicality of acquiring chest X-ray \n",
            "images by patients, we also developed a web application (link: https://toad.li/xray) based on our model to directly enable users \n",
            "to upload chest X-ray images and detect the presence of COVID -19 within a few seconds. Taken together, here we introduce \n",
            "a state-of-the-art artificial intelligence-based system for efficient COVID-19 detection and a user-friendly application that has \n",
            "the capacity to become a rapid COVID-19 diagnosis method in the near future.\n",
            " \n",
            "Keywords: COVID-19, Coronavirus detection, Deep learning, X-ray, Pneumonia, Classification \n",
            " \n",
            "1. INTRODUCTION    \n",
            "The COVID-19 is a viral infection that causes severe \n",
            "respiratory illness ranging from common cold to life \n",
            "threating diseases like Severe Acute Respiratory Syndrome \n",
            "(SARS) and Middle East Respiratory Syndrome (MERS). \n",
            "According to reports from the World Health Organization \n",
            "(WHO), major symptoms of COVID -19 are s imilar to that \n",
            "of common flu: fever, tiredness, dry cough, shortness of \n",
            "breath, aches and sore throat [1,2]. The similarities between \n",
            "COVID-19 and flu symptoms causes difficulties in detection \n",
            "of the coro navirus at early stages. It was found in some \n",
            "patients that the coronavirus, like other viruses and bacteria, \n",
            "also causes pneumonia, and the treatment for coronavirus \n",
            "induced pneumonia is different from other types of \n",
            "pneumonia. Moreover, bacterial pneumon ia infected \n",
            "patients require antibiotic treatment whereas viral \n",
            "pneumonia cases can be treated by intensive care [3]. \n",
            "Therefore, accurate and timely diagnosis of COVID -19 \n",
            "induced pneumonia is very important to save human lives as \n",
            "well as curbing the pandemic outbreak across the world. \n",
            "The WHO approved method of testing COVID -19 is \n",
            "the reverse transcription polymerase chain reaction (RT -\n",
            "PCR) where short sequences of RNA are analyzed to detect\n"
          ]
        }
      ],
      "source": [
        "#view page content\n",
        "print(pages[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyDp0eJj_hNd"
      },
      "source": [
        "## <font color=blue> TASK 2: Define the summarize pdf function\n",
        "Define the main function that will take pdf file path as an input and generate a summary of the file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kON5tJmk09Mp"
      },
      "outputs": [],
      "source": [
        "# def summarize_pdf(pdf_file_path, chunk_size, chunk_overlap):\n",
        "\n",
        "#     #Instantiate LLM model gpt-3.5-turbo-16k\n",
        "#     llm=ChatOpenAI(model=\"tngtech/deepseek-r1t-chimera:free\", temperature=0, openai_api_key=OpenAI.api_key)\n",
        "\n",
        "#     #Load PDF file\n",
        "#     loader = PyPDFLoader(pdf_file_path)\n",
        "#     docs_raw = loader.load()\n",
        "\n",
        "#     #Create multiple documents\n",
        "#     docs_raw_text = [doc.page_content for doc in docs_raw]\n",
        "\n",
        "#     #Split text into chunks\n",
        "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "#     docs_chunks = text_splitter.create_documents(docs_raw_text)\n",
        "\n",
        "#     #Summarize the chunks (map_reduce method takes longer to execute)\n",
        "#     chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "#     #Return the summary\n",
        "#     summary = chain.invoke(docs_chunks, return_only_outputs=True)\n",
        "#     return summary['output_text']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from openai import OpenAI\n",
        "# from langchain.document_loaders import PyPDFLoader\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# def summarize_pdf(pdf_file_path, chunk_size, chunk_overlap):\n",
        "#     # Initialize OpenAI client (using OpenRouter)\n",
        "#     client = OpenAI(\n",
        "#         base_url=\"https://openrouter.ai/api/v1\",\n",
        "#         api_key= userdata.get('OPENAI_API_KEY'),\n",
        "#     )\n",
        "\n",
        "#     # Load the PDF\n",
        "#     loader = PyPDFLoader(pdf_file_path)\n",
        "#     docs_raw = loader.load()\n",
        "#     docs_raw_text = [doc.page_content for doc in docs_raw]\n",
        "\n",
        "#     # Split text into chunks\n",
        "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "#     docs_chunks = text_splitter.create_documents(docs_raw_text)\n",
        "\n",
        "#     # Prepare the input for summarization\n",
        "#     summaries = []\n",
        "#     for chunk in docs_chunks:\n",
        "#         message = [\n",
        "#             {\"role\": \"user\", \"content\": f\"Please summarize the following:\\n\\n{chunk.page_content}\"}\n",
        "#         ]\n",
        "#         completion = client.chat.completions.create(\n",
        "#             model=\"tngtech/deepseek-r1t-chimera:free\",  # <-- You can change the model here\n",
        "#             messages=message,\n",
        "#             extra_headers={\n",
        "#                 \"HTTP-Referer\": \"<YOUR_SITE_URL>\", # optional\n",
        "#                 \"X-Title\": \"<YOUR_SITE_NAME>\",     # optional\n",
        "#             }\n",
        "#         )\n",
        "#         summary_text = completion.choices[0].message.content\n",
        "#         summaries.append(summary_text)\n",
        "\n",
        "#     # Optionally: merge all chunk summaries into one big text\n",
        "#     final_summary = \"\\n\\n\".join(summaries)\n",
        "\n",
        "#     return final_summary\n"
      ],
      "metadata": {
        "id": "2jdZDHYGkLgH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define your API key\n",
        "OPENROUTER_API_KEY =  userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "def summarize_pdf(pdf_file_path, chunk_size, chunk_overlap):\n",
        "    # Instantiate LLM model using OpenRouter\n",
        "    llm = ChatOpenAI(\n",
        "        openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "        openai_api_key=OPENROUTER_API_KEY,\n",
        "        model_name=\"google/gemma-3-1b-it:free\",\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "    # Load PDF file\n",
        "    loader = PyPDFLoader(pdf_file_path)\n",
        "    docs_raw = loader.load()\n",
        "\n",
        "    # Create multiple documents\n",
        "    docs_raw_text = [doc.page_content for doc in docs_raw]\n",
        "\n",
        "    # Split text into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    docs_chunks = text_splitter.create_documents(docs_raw_text)\n",
        "\n",
        "    # Summarize the chunks (stuff method is faster)\n",
        "    chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "    # chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "\n",
        "    # Return the summary\n",
        "    summary = chain.invoke(docs_chunks, return_only_outputs=True)\n",
        "    return summary['output_text']"
      ],
      "metadata": {
        "id": "tPu4hRuxnSW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import subprocess\n",
        "# import json\n",
        "# from langchain.document_loaders import PyPDFLoader\n",
        "# from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "# GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "\n",
        "# def call_groq_api(prompt):\n",
        "#     data = {\n",
        "#         \"model\": \"deepseek-r1-distill-llama-70b\",\n",
        "#         \"messages\": [{\n",
        "#             \"role\": \"user\",\n",
        "#             \"content\": prompt\n",
        "#         }]\n",
        "#     }\n",
        "\n",
        "#     curl_command = [\n",
        "#         \"curl\", \"https://api.groq.com/openai/v1/chat/completions\",\n",
        "#         \"-s\",\n",
        "#         \"-H\", \"Content-Type: application/json\",\n",
        "#         \"-H\", f\"Authorization: Bearer {GROQ_API_KEY}\",\n",
        "#         \"-d\", json.dumps(data)\n",
        "#     ]\n",
        "\n",
        "#     result = subprocess.run(curl_command, capture_output=True, text=True)\n",
        "\n",
        "#     try:\n",
        "#         response = json.loads(result.stdout)\n",
        "#     except json.JSONDecodeError:\n",
        "#         print(\"Failed to decode JSON. Raw output:\", result.stdout)\n",
        "#         raise\n",
        "\n",
        "#     if \"choices\" not in response:\n",
        "#         print(\"Error from GROQ API:\", response)\n",
        "#         raise Exception(\"GROQ API call failed.\")\n",
        "\n",
        "#     return response['choices'][0]['message']['content']\n",
        "\n",
        "# def summarize_pdf(pdf_file_path, chunk_size, chunk_overlap):\n",
        "#     # Load PDF\n",
        "#     loader = PyPDFLoader(pdf_file_path)\n",
        "#     docs_raw = loader.load()\n",
        "\n",
        "#     # Extract text from pages\n",
        "#     docs_raw_text = [doc.page_content for doc in docs_raw]\n",
        "\n",
        "#     # Split text into chunks\n",
        "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "#     docs_chunks = text_splitter.create_documents(docs_raw_text)\n",
        "\n",
        "#     # Summarize each chunk separately\n",
        "#     all_summaries = []\n",
        "#     for idx, chunk in enumerate(docs_chunks):\n",
        "#         print(f\"Summarizing chunk {idx+1}/{len(docs_chunks)}...\")\n",
        "#         summary = call_groq_api(chunk.page_content)\n",
        "#         all_summaries.append(summary)\n",
        "\n",
        "#     # Combine all small summaries\n",
        "#     final_summary = \"\\n\\n\".join(all_summaries)\n",
        "#     return final_summary\n"
      ],
      "metadata": {
        "id": "1_XdeFkWoscn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jP624HFsZyT"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zZsZ5TGMM-WS",
        "outputId": "30947e4d-4037-4db8-9ee6-c321dc736662"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have analyzed the provided text and created a concise summary.\n",
            "\n",
            "The research focuses on developing a deep learning model for detecting COVID-19 from chest X-ray images. The model, based on pre-trained convolutional neural networks (CNNs), demonstrates high accuracy in classifying images, surpassing existing methods. The study highlights the potential of AI to assist in rapid diagnosis and improve patient outcomes.  The research also explores the challenges of model generalization and the need for further refinement through techniques like transfer learning and data augmentation.\n"
          ]
        }
      ],
      "source": [
        "#print summary by using chain type stuff or map_reduce\n",
        "#Chunk size and chunk overlap values set to random value\n",
        "\n",
        "print(summarize_pdf(pdf_url, 1000, 20))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cvnt8H-L3Yzb"
      },
      "source": [
        "## <font color=blue> TASK 3: Add Prompt template to the summarizer function\n",
        "Leveraging prompt templates to extract key information from the reserach paper in more guided manner."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_s2a87sV4Hkl"
      },
      "source": [
        "## <font color=blue> Define Prompt Templates"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wy1_iJlfQRR"
      },
      "source": [
        "### <font color=black> Prompt Template for Stuffing chain type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zac7tS6p3YKU"
      },
      "outputs": [],
      "source": [
        "map_prompt_template = \"\"\"\n",
        "                       Write a summary of the research paper for an\n",
        "                       artficial intelligence researcher that includes\n",
        "                       main points and any important details in bullet points.{text}\n",
        "                      \"\"\"\n",
        "\n",
        "# map_prompt_template = \"\"\"\n",
        "#                        Write a summary of the research paper for a\n",
        "#                        high school student to provide a non-technical overview\n",
        "#                        of the research topic in bullet points.{text}\n",
        "#                       \"\"\"\n",
        "\n",
        "map_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=map_prompt_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il7HTR6pfjsF"
      },
      "source": [
        "### <font color=black> Add Combo Template for Map_Reduce chain type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AaZoSIMDfMPu"
      },
      "outputs": [],
      "source": [
        "combine_prompt_template = \"\"\"\n",
        "                          You will be given main points and any important details of a research paper in bullet points.\n",
        "                          Your goal is to give a final summary of the main research topic and findings\n",
        "                          which will be useful to an artificial intelligence researcher\n",
        "                          to grasp what was done during the research work.\n",
        "```{text}```\n",
        "FINAL SUMMARY:\n",
        "\"\"\"\n",
        "\n",
        "combine_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=map_prompt_template,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xyYdjJZEJJs"
      },
      "outputs": [],
      "source": [
        "# #Modify the custom function to add the prompt templates\n",
        "\n",
        "# def summarize_pdf(pdf_file_path, chunk_size, chunk_overlap):\n",
        "\n",
        "#     #Instantiate LLM model gpt-3.5-turbo-16k\n",
        "#     llm=ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0, openai_api_key=OpenAI.api_key)\n",
        "\n",
        "#     #Load PDF file\n",
        "#     loader = PyPDFLoader(pdf_file_path)\n",
        "#     docs_raw = loader.load()\n",
        "\n",
        "#     #Create multiple documents\n",
        "#     docs_raw_text = [doc.page_content for doc in docs_raw]\n",
        "\n",
        "#     #Split text into chunks\n",
        "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "#     docs_chunks = text_splitter.create_documents(docs_raw_text)\n",
        "\n",
        "#     #Summarize the chunks\n",
        "#     chain = load_summarize_chain(llm, chain_type=\"stuff\")\n",
        "\n",
        "\n",
        "#     #Return the summary\n",
        "#     summary = chain.invoke(docs_chunks, return_only_outputs=True)\n",
        "#     return summary['output_text']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_pdf(pdf_file_path, chunk_size, chunk_overlap, map_prompt):\n",
        "    # Instantiate LLM model using OpenRouter\n",
        "    llm = ChatOpenAI(\n",
        "        openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "        openai_api_key=OPENROUTER_API_KEY,\n",
        "        model_name=\"google/gemma-3-1b-it:free\",\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "    # Load PDF file\n",
        "    loader = PyPDFLoader(pdf_file_path)\n",
        "    docs_raw = loader.load()\n",
        "\n",
        "    # Create multiple documents\n",
        "    docs_raw_text = [doc.page_content for doc in docs_raw]\n",
        "\n",
        "    # Split text into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    docs_chunks = text_splitter.create_documents(docs_raw_text)\n",
        "\n",
        "    # Summarize the chunks (stuff method is faster)\n",
        "    chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=map_prompt)\n",
        "    # chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "\n",
        "    # Return the summary\n",
        "    summary = chain.invoke(docs_chunks, return_only_outputs=True)\n",
        "    return summary['output_text']"
      ],
      "metadata": {
        "id": "whZ8Idmlty0A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2bNEpmcPG0w",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8402e6cb-8807-411d-8077-4e9d4a12f101"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I’ve revised the response to better address the prompt and provide a more comprehensive overview. Here’s a revised version, incorporating the requested elements and aiming for clarity and detail:\n",
            "\n",
            "**COVID-19 Chest X-Ray Image Classification Using Deep Learning**\n",
            "\n",
            "**Abstract**\n",
            "\n",
            "The rapid spread of COVID-19 has underscored the critical need for accurate and timely diagnostic tools. Chest X-ray imaging is a valuable modality for detecting pneumonia and other respiratory illnesses. This paper presents a state-of-the-art deep learning approach for classifying chest X-ray images, aiming to improve the efficiency and accuracy of COVID-19 diagnosis. We detail the methodology, including pre-trained model selection, model training, and performance evaluation.\n",
            "\n",
            "**1. Introduction**\n",
            "\n",
            "The emergence of COVID-19 has triggered a global pandemic, necessitating rapid diagnostic capabilities. Chest X-ray imaging is a routinely performed diagnostic tool, but its interpretation can be challenging, particularly in the early stages of infection.  Early detection and accurate classification are crucial for effective treatment and resource allocation.  This research addresses the challenge of automating the detection of COVID-19 from chest X-ray images, offering a potential solution for improved clinical workflow.\n",
            "\n",
            "**2. Related Work**\n",
            "\n",
            "Previous studies have explored various deep learning techniques for medical image analysis, including convolutional neural networks (CNNs).  Several research groups have demonstrated the effectiveness of CNNs in classifying chest X-rays for various respiratory diseases, including pneumonia and COVID-19.  However, many existing methods suffer from limitations such as high computational cost, limited data availability, and difficulty in generalizing to diverse patient populations.\n",
            "\n",
            "**3. Methodology**\n",
            "\n",
            "Our approach leverages a pre-trained deep learning model as a foundation. We employed the following steps:\n",
            "\n",
            "*   **Model Selection:** We selected ResNet-50, a widely used CNN architecture known for its performance and efficiency.  We also considered VGG16 and EfficientNet, but ResNet-50 proved to be the most effective for this task.\n",
            "*   **Data Preprocessing:** The dataset comprised 6,432 chest X-ray images, categorized into three classes: (1) COVID-19, (2) Pneumonia, and (3) Normal/Healthy.  Images were preprocessed to ensure consistent image dimensions and quality.\n",
            "*   **Transfer Learning:**  We utilized transfer learning, leveraging pre-trained weights from ImageNet to accelerate training and improve performance.  The pre-trained model was fine-tuned on the COVID-19 dataset.\n",
            "*   **Model Architecture:** We implemented a multi-stage architecture:\n",
            "    *   **Stage 1:**  A convolutional block to extract initial features.\n",
            "    *   **Stage 2:**  A series of residual blocks to learn hierarchical features.\n",
            "    *   **Stage 3:**  A final classification layer with a sigmoid activation function to produce a probability score.\n",
            "*   **Hyperparameter Optimization:** We employed a grid search to optimize hyperparameters such as learning rate, batch size, and number of epochs.\n",
            "\n",
            "**4. Results and Discussion**\n",
            "\n",
            "The trained model achieved an accuracy of 96.7% on the test dataset.  The model demonstrated high performance in distinguishing between COVID-19 and pneumonia cases.  The model's performance was further evaluated by analyzing the precision, recall, and F1-score for each class.  The model's performance was also evaluated on a validation set to ensure that the model was generalizing well to unseen data.\n",
            "\n",
            "**5. Conclusion**\n",
            "\n",
            "This study demonstrates the potential of deep learning to improve the accuracy and efficiency of COVID-19 diagnosis through chest X-ray image classification.  The proposed methodology offers a robust and scalable solution for automated detection, potentially reducing the workload of radiologists and improving patient outcomes.\n",
            "\n",
            "**6.  Future Work**\n",
            "\n",
            "Future research directions include:\n",
            "\n",
            "*   Exploring different pre-trained models and fine-tuning strategies.\n",
            "*   Incorporating additional features, such as lung nodule detection, to enhance diagnostic accuracy.\n",
            "*   Developing a user-friendly interface for seamless integration into clinical workflows.\n",
            "*   Investigating the use of federated learning to train models on decentralized datasets.\n",
            "\n",
            "**7.  Acknowledgements**\n",
            "\n",
            "We would like to thank Prashanth Patel for creating the C OVID-19 dataset and making it publicly accessible on the Kaggle website. We also thank Debosmita Sardar, PhD for providing feedback during manuscript preparation. We thank Dr. Jesamar Mattos for providing feedback on the efficacy of the DLH_COVID model during final accuracy check.\n",
            "\n",
            "**References**\n",
            "\n",
            "(A comprehensive list of relevant research papers would be included here.  This is a placeholder.)\n",
            "\n",
            "**Note:** This response provides a more detailed and structured overview of the research.  It includes a clear explanation of the methodology, results, and potential future directions.  Remember to replace the placeholder references with actual citations from relevant publications.  Also, the inclusion of a comprehensive list of references is crucial for a robust research paper.\n"
          ]
        }
      ],
      "source": [
        "#print summary using the map_prompt and combine prompt\n",
        "#Increasing the chunk size value might reduce the overall summarization time with map_reduce method\n",
        "print(summarize_pdf(pdf_url, 2000, 100, map_prompt))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_pdf(pdf_file_path, chunk_size, chunk_overlap, map_prompt, combine_prompt):\n",
        "    # Instantiate LLM model using OpenRouter\n",
        "    llm = ChatOpenAI(\n",
        "        openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "        openai_api_key=OPENROUTER_API_KEY,\n",
        "        model_name=\"google/gemma-3-1b-it:free\",\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "    # Load PDF file\n",
        "    loader = PyPDFLoader(pdf_file_path)\n",
        "    docs_raw = loader.load()\n",
        "\n",
        "    # Create multiple documents\n",
        "    docs_raw_text = [doc.page_content for doc in docs_raw]\n",
        "\n",
        "    # Split text into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    docs_chunks = text_splitter.create_documents(docs_raw_text)\n",
        "\n",
        "    # Summarize the chunks (stuff method is faster)\n",
        "    chain = load_summarize_chain(llm, chain_type=\"map_reduce\", map_prompt= map_prompt, combine_prompt=combine_prompt)\n",
        "    # chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "\n",
        "    # Return the summary\n",
        "    summary = chain.invoke(docs_chunks, return_only_outputs=True)\n",
        "    return summary['output_text']"
      ],
      "metadata": {
        "id": "8GDeQy02v7Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print summary using the map_prompt and combine prompt\n",
        "#Increasing the chunk size value might reduce the overall summarization time with map_reduce method\n",
        "print(summarize_pdf(pdf_url, 2000, 100, map_prompt, combine_prompt))"
      ],
      "metadata": {
        "id": "UkrXIGwnv-wf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XUnEVtU849_"
      },
      "source": [
        "## <font color=blue> TASK 4: Build and test a GenAI app for PDF summarization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "urhXeIwaS6RN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc43361a-fef9-476f-b859-feaaf6f0c357"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import dotenv\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import streamlit as st\n",
        "\n",
        "\n",
        "# Load the .env file and invoke the secret API key from the file\n",
        "dotenv.load_dotenv('API.env')\n",
        "# OpenAI.api_key = os.getenv(\"OPEN_API_KEY\")\n",
        "OPENROUTER_API_KEY = os.getenv(\"OPEN_API_KEY\")\n",
        "\n",
        "#summarize_pdf function\n",
        "\n",
        "# def summarize_pdf(pdf_file_path, chunk_size, chunk_overlap, prompt):\n",
        "#     #Instantiate LLM model gpt-3.5-turbo-16k\n",
        "#     llm=ChatOpenAI(model=\"gpt-3.5-turbo-16k\", temperature=0, openai_api_key=OpenAI.api_key)\n",
        "\n",
        "#     #Load PDF file\n",
        "#     loader = PyPDFLoader(pdf_file_path)\n",
        "#     docs_raw = loader.load()\n",
        "\n",
        "#     #Create multiple documents\n",
        "#     docs_raw_text = [doc.page_content for doc in docs_raw]\n",
        "\n",
        "#     #Split text into chunks\n",
        "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "#     docs_chunks = text_splitter.create_documents(docs_raw_text)\n",
        "\n",
        "#     #Summarize the chunks\n",
        "#     chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt = prompt)\n",
        "#     #Return the summary\n",
        "#     summary = chain.invoke(docs_chunks, return_only_outputs=True)\n",
        "#     return summary['output_text']\n",
        "def summarize_pdf(pdf_file_path, chunk_size, chunk_overlap, map_prompt):\n",
        "    # Instantiate LLM model using OpenRouter\n",
        "    llm = ChatOpenAI(\n",
        "        openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "        openai_api_key=OPENROUTER_API_KEY,\n",
        "        model_name=\"google/gemma-3-1b-it:free\",\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "    # Load PDF file\n",
        "    loader = PyPDFLoader(pdf_file_path)\n",
        "    docs_raw = loader.load()\n",
        "\n",
        "    # Create multiple documents\n",
        "    docs_raw_text = [doc.page_content for doc in docs_raw]\n",
        "\n",
        "    # Split text into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    docs_chunks = text_splitter.create_documents(docs_raw_text)\n",
        "\n",
        "    # Summarize the chunks (stuff method is faster)\n",
        "    chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=map_prompt)\n",
        "    # chain = load_summarize_chain(llm, chain_type=\"map_reduce\")\n",
        "\n",
        "    # Return the summary\n",
        "    summary = chain.invoke(docs_chunks, return_only_outputs=True)\n",
        "    return summary['output_text']\n",
        "\n",
        "#streamlit app main() function\n",
        "\n",
        "def main():\n",
        "    #Set page config and title\n",
        "    st.set_page_config(page_title=\"PDF Summarizer\", page_icon=\":book:\", layout=\"wide\")\n",
        "    st.title(\"PDF Summarizer\")\n",
        "    #Input pdf file path\n",
        "    pdf_file_path = st.text_input(\"Enter the path to the PDF file:\")\n",
        "    if pdf_file_path != \"\":\n",
        "      st.write(\"PDF file was loaded successfully\")\n",
        "\n",
        "    #prompt input\n",
        "    user_prompt = st.text_input(\"Enter your prompt:\")\n",
        "    user_prompt = user_prompt + \"\"\"{text}\"\"\"\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"text\"],\n",
        "        template=user_prompt,\n",
        "    )\n",
        "    #Summarize button\n",
        "    if st.button(\"Summarize\"):\n",
        "      summary = summarize_pdf(pdf_file_path, 1000, 20, prompt)\n",
        "      st.write(summary)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10C8V-CIL4Ti"
      },
      "source": [
        "## <font color=blue> Launch Streamlit app from Google Colab\n",
        "\n",
        "The following lines of code would enable users to launch Streamlit app from Google Colab using [ngrok service](https://ngrok.com/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaPE0T2iDzwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8dea763a-ab53-4cde-8e56-f1de569c09af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-28 19:47:23--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 13.248.244.96, 99.83.220.108, 75.2.60.68, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|13.248.244.96|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 13921656 (13M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]  13.28M  33.7MB/s    in 0.4s    \n",
            "\n",
            "2025-04-28 19:47:24 (33.7 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [13921656/13921656]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Brgmtoc7CVKx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d55f56d-b5e5-48e5-b924-a4c886c0e94a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ]
        }
      ],
      "source": [
        "!unzip ngrok-stable-linux-amd64.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hle-PIMoQfLs"
      },
      "outputs": [],
      "source": [
        "get_ipython().system_raw('./ngrok http 8501 &')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z7C6dydMS06y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35e3cecd-1657-442a-e55a-5259404964bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.55.173.42\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1Rduaz0TG-Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1cc5f877-4496-4acf-fd91-8a8585ff79bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.55.173.42:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0Kyour url is: https://empty-suns-fail.loca.lt\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJtA0iPfgBn_"
      },
      "source": [
        "## <font color=blue> FINAL TASK: Cumulative Activity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFTCcg_COEie"
      },
      "source": [
        "Click the link to explore useful Streamlit library functions:\n",
        "https://docs.streamlit.io/library/cheatsheet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PiEdC80-gP-k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de218bfe-af4b-447f-e7f2-580f7a08c080"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import os\n",
        "import dotenv\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_openai.chat_models import ChatOpenAI\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "import streamlit as st\n",
        "\n",
        "# Load the .env file and invoke the secret API key from the file\n",
        "dotenv.load_dotenv('API.env')\n",
        "OPENROUTER_API_KEY = os.getenv(\"OPEN_API_KEY\")\n",
        "\n",
        "#summarize_pdf function\n",
        "\n",
        "def summarize_pdf(pdf_file_path, chunk_size, chunk_overlap, chain_type, prompt):\n",
        "    #Instantiate LLM model gpt-3.5-turbo-16k\n",
        "    llm = ChatOpenAI(\n",
        "        openai_api_base=\"https://openrouter.ai/api/v1\",\n",
        "        openai_api_key=OPENROUTER_API_KEY,\n",
        "        model_name=\"google/gemma-3-1b-it:free\",\n",
        "        temperature=0,\n",
        "    )\n",
        "\n",
        "    #Load PDF file\n",
        "    loader = PyPDFLoader(pdf_file_path)\n",
        "    docs_raw = loader.load()\n",
        "\n",
        "    #Create multiple documents\n",
        "    docs_raw_text = [doc.page_content for doc in docs_raw]\n",
        "\n",
        "    #Split text into chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    docs_chunks = text_splitter.create_documents(docs_raw_text)\n",
        "\n",
        "    #Create multiple prompts\n",
        "    prompt = prompt + \"\"\"{text}\"\"\"\n",
        "    combine_prompt = PromptTemplate(input_variables=[\"text\"], template=prompt)\n",
        "    map_prompt = PromptTemplate(template=\"Summarize in bullet points:\\n\\n{text}\", input_variables=[\"text\"])\n",
        "\n",
        "    #Summarize the chunks\n",
        "    if chain_type == \"map_reduce\":\n",
        "      chain = load_summarize_chain(llm, chain_type=chain_type,\n",
        "                                   map_prompt=map_prompt, combine_prompt=combine_prompt)\n",
        "    else:\n",
        "      chain = load_summarize_chain(llm, chain_type=chain_type,\n",
        "                                   prompt=map_prompt)\n",
        "    #Return the summary\n",
        "    return chain.run(docs_chunks)\n",
        "\n",
        "#streamlit app main() function\n",
        "\n",
        "def main():\n",
        "    #Set page config and title\n",
        "    st.set_page_config(page_title=\"PDF Summarizer\", page_icon=\":book:\", layout=\"wide\")\n",
        "    st.title(\"Wassim's GenAI App\")\n",
        "\n",
        "    #Add custom sliders and selectbox for more user interaction\n",
        "    chain_type = st.sidebar.selectbox(\"Chain Type\", [\"stuff\", \"map_reduce\"])\n",
        "    chunk_size = st.sidebar.slider(\"Chunk Size\", min_value=100, max_value=10000, step=100, value=1000)\n",
        "    chunk_overlap = st.sidebar.slider(\"Chunk Overlap\", min_value=10, max_value=1000, step=100, value=20)\n",
        "\n",
        "    #Display warning message\n",
        "    if 'map_reduce' in chain_type:\n",
        "      st.sidebar.warning (f'Map reduce chain type takes more than 5 mins to generate summary due to prompt latency!')\n",
        "\n",
        "    #Input pdf file path\n",
        "    pdf_file_path = st.text_input(\"Enter PDF file path:\")\n",
        "\n",
        "    #Prompt input\n",
        "    user_prompt = st.text_input(\"Enter prompt:\")\n",
        "\n",
        "    #Summarize button\n",
        "    if st.button(\"Summarize\"):\n",
        "        #Summarize pdf\n",
        "        summary = summarize_pdf(pdf_file_path, chunk_size, chunk_overlap, chain_type, user_prompt)\n",
        "        st.write(summary)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xn7-oPQOPNUD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d5eae0f-18de-43fa-9144-c45a0151c6ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.55.173.42\n"
          ]
        }
      ],
      "source": [
        "!wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "141g6WFTPVbM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cbd7729-f285-4c48-c2cd-b2f71b610243"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.55.173.42:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K⠇\u001b[1G\u001b[0K⠏\u001b[1G\u001b[0Kyour url is: https://clean-birds-raise.loca.lt\n",
            "/content/app.py:50: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  return chain.run(docs_chunks)\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6WlwSgMKPp7q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}